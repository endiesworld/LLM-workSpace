{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a19c991-8fc4-4400-b0ea-c24b33f3a720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d056d867-cb5d-4872-b8c3-15db4e845318",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502eff7a-3da1-4255-bdbe-c50f1427c337",
   "metadata": {},
   "source": [
    "## Preparation \n",
    "\n",
    "- Define a function that we will use when building our agent.\n",
    "- It will generate fake weather data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca67b391-228e-4791-8794-90667401b881",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_weather_data = {\n",
    "    'berlin': 20.0\n",
    "}\n",
    "\n",
    "def get_weather(city: str) -> float:\n",
    "    city = city.strip().lower()\n",
    "\n",
    "    if city in known_weather_data:\n",
    "        return known_weather_data[city]\n",
    "\n",
    "    return round(random.uniform(-5, 35), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dc038d-0b88-4699-925c-e94deddcc9a0",
   "metadata": {},
   "source": [
    "## Q1. Define function description\n",
    "We want to use it as a tool for our agent, so we need to describe it\n",
    "\n",
    "```python\n",
    "get_weather_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"<TODO1>\",\n",
    "    \"description\": \"<TODO2>\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"<TODO3>\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"<TODO4>\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [TODO5],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "170db020-6cb8-4c68-a814-dd44f6bed2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_weather_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"get_weather\",\n",
    "    \"description\": \"Get weather data for a given city.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"city\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The city whose weather data is required\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"city\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf39255-a130-4e14-8d6d-5c0d99ffde04",
   "metadata": {},
   "source": [
    "## Define a System and User Prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b1992952-4cec-4813-8126-591800cdec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#system prompt\n",
    "system_prompt = \"\"\"You are an intelligent, autonomous assistant capable of reasoning, planning, and taking actions by calling external tools when needed. Your goal is to fulfill user requests accurately and efficiently.\n",
    "\n",
    "You have access to the following tool:\n",
    "- `get_weather`: Use this to get current weather information for a given city. It accepts a parameter `query`, which is a string representing the city name.\n",
    "\n",
    "Your behavior:\n",
    "- Decide whether a tool call is needed to answer the user’s query.\n",
    "- If a tool call is necessary, construct the function call with accurate parameters.\n",
    "- Once you receive the tool's response, interpret it correctly and present the final answer to the user.\n",
    "- Respond in a clear and informative way, tailored to the user's original intent.\n",
    "- Ask clarifying questions only when necessary and aim to complete the task in minimal steps.\n",
    "\n",
    "Constraints:\n",
    "- Do not fabricate information if a tool call is required to retrieve it.\n",
    "- Do not ask the user for information that can be directly obtained using available tools.\n",
    "- Always provide your final answer after using a tool, even if the response is short.\n",
    "\n",
    "Examples of appropriate tool use:\n",
    "- User: \"What's the weather in London?\" → Call `weather` with `query=\"London\"`.\n",
    "- User: \"Should I carry an umbrella in New York?\" → Call `weather` with `query=\"New York\"` and interpret rain condition.\n",
    "- User: \"How hot is it in Paris today?\" → Call `weather` with `query=\"Paris\"` and extract temperature.\n",
    "\n",
    "You are proactive, helpful, and reliable. Make decisions like an intelligent assistant that can take initiative.\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ef9ecfd0-b9f6-48dc-9f56-f48bc2565524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User prompt\n",
    "user_question = \"What is the weather in New Jersey today?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204c1a13-d45f-4ac6-a1ca-c73e6815e030",
   "metadata": {},
   "source": [
    "## Build chat message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "939c7bc0-71dc-49e0-ad97-bbd2051eee5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agent chat message\n",
    "chat_messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_question}\n",
    "               ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341fe425-814a-4066-9734-b0402c945ddb",
   "metadata": {},
   "source": [
    "## Create tool list and add weather tool to the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0c325a31-81e9-44fa-aef5-d089c7e91a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [get_weather_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d8cb41-2cf3-417b-8851-268b8b10d26a",
   "metadata": {},
   "source": [
    "## Call the LLM API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b55fce6f-9c4f-4ded-8d28-fd594e6bc3b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ResponseFunctionToolCall(arguments='{\"city\":\"New Jersey\"}', call_id='call_21L3kjAx0Gqu5Gfz1HbCxrrY', name='get_weather', type='function_call', id='fc_6876b31e64208198b9ed23458a0519ee0cf925ef713baf44', status='completed')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=chat_messages,\n",
    "    tools=tools\n",
    ")\n",
    "response.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843ea11e-7482-4644-a760-1ce999f5b8c6",
   "metadata": {},
   "source": [
    "# Take action based on LLM response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b7702102-d536-4d80-89fe-aaa6560d0263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(id='resp_6876b31d8a5481989e89ecf5afe175920cf925ef713baf44', created_at=1752609565.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[ResponseFunctionToolCall(arguments='{\"city\":\"New Jersey\"}', call_id='call_21L3kjAx0Gqu5Gfz1HbCxrrY', name='get_weather', type='function_call', id='fc_6876b31e64208198b9ed23458a0519ee0cf925ef713baf44', status='completed')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FunctionTool(name='get_weather', parameters={'type': 'object', 'properties': {'city': {'type': 'string', 'description': 'The city whose weather data is required'}}, 'required': ['city'], 'additionalProperties': False}, strict=True, type='function', description='Get weather data for a given city.')], top_p=1.0, background=False, max_output_tokens=None, previous_response_id=None, prompt=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=ResponseUsage(input_tokens=382, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=16, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=398), user=None, max_tool_calls=None, store=True, top_logprobs=0)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7c2bec25-8239-4090-b1b0-1fc3012ed15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "llm_responses =  response.output\n",
    "function_names = []\n",
    "arguments = []\n",
    "\n",
    "for response_ in llm_responses:\n",
    "    response_type = response_.type\n",
    "    response_id = response_.call_id\n",
    "\n",
    "    if response_type == \"function_call\":\n",
    "        function_names.append(response_.name)\n",
    "        arguments.append(json.loads(response_.arguments))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace1b207-d1a1-46d8-ba7b-8d197ff9dcc4",
   "metadata": {},
   "source": [
    "## Check to see if LLM wants to use a tool:\n",
    "- Call the functions with the returned arguments\n",
    "- Update the chat messages manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "60018829-3c57-45b4-a73b-4c19c0993108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function call: get_weather, argument: {'city': 'New Jersey'}\n",
      "result: 11.7\n"
     ]
    }
   ],
   "source": [
    "functions_result = []\n",
    "for function_name, argument in zip( function_names, arguments):\n",
    "    function = globals()[function_name]\n",
    "    result = function(**argument)\n",
    "    functions_result.append(result)\n",
    "    print(f\"Function call: {function_name}, argument: {argument}\\nresult: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b70ccae-ad71-402b-94b4-701f8d50416e",
   "metadata": {},
   "source": [
    "## Call LLM again with updated chat message containing the result from the tool call. \n",
    "- Save both the response and the result of the function call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6c2caded-0768-4d91-b379-3c8306337f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11.7'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.dumps(functions_result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c0fcfdf5-51e8-4ed4-b937-d71907209a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are an intelligent, autonomous assistant capable of reasoning, planning, and taking actions by calling external tools when needed. Your goal is to fulfill user requests accurately and efficiently.\\n\\nYou have access to the following tool:\\n- `get_weather`: Use this to get current weather information for a given city. It accepts a parameter `query`, which is a string representing the city name.\\n\\nYour behavior:\\n- Decide whether a tool call is needed to answer the user’s query.\\n- If a tool call is necessary, construct the function call with accurate parameters.\\n- Once you receive the tool\\'s response, interpret it correctly and present the final answer to the user.\\n- Respond in a clear and informative way, tailored to the user\\'s original intent.\\n- Ask clarifying questions only when necessary and aim to complete the task in minimal steps.\\n\\nConstraints:\\n- Do not fabricate information if a tool call is required to retrieve it.\\n- Do not ask the user for information that can be directly obtained using available tools.\\n- Always provide your final answer after using a tool, even if the response is short.\\n\\nExamples of appropriate tool use:\\n- User: \"What\\'s the weather in London?\" → Call `weather` with `query=\"London\"`.\\n- User: \"Should I carry an umbrella in New York?\" → Call `weather` with `query=\"New York\"` and interpret rain condition.\\n- User: \"How hot is it in Paris today?\" → Call `weather` with `query=\"Paris\"` and extract temperature.\\n\\nYou are proactive, helpful, and reliable. Make decisions like an intelligent assistant that can take initiative.'},\n",
       " {'role': 'user', 'content': 'What is the weather in New Jersey today?'},\n",
       " ResponseFunctionToolCall(arguments='{\"city\":\"New Jersey\"}', call_id='call_21L3kjAx0Gqu5Gfz1HbCxrrY', name='get_weather', type='function_call', id='fc_6876b31e64208198b9ed23458a0519ee0cf925ef713baf44', status='completed'),\n",
       " {'type': 'function_call_output',\n",
       "  'call_id': 'call_21L3kjAx0Gqu5Gfz1HbCxrrY',\n",
       "  'output': '11.7'}]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for response_, result in zip(llm_responses, functions_result):\n",
    "    chat_messages.append(response_)\n",
    "    \n",
    "    chat_messages.append({\n",
    "        \"type\": \"function_call_output\",\n",
    "        \"call_id\": response_.call_id,\n",
    "        \"output\": json.dumps(result),\n",
    "    })\n",
    "\n",
    "chat_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6af344-93bc-40da-ae7e-3cff58c0ba6d",
   "metadata": {},
   "source": [
    "Now chat_messages contains both the call description (so it keeps track of history) and the results\n",
    "\n",
    "Let's make another call to the model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "65c9fed2-931d-4b63-8eb3-4db8f30c0ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=chat_messages,\n",
    "    tools=tools\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f3c56b39-2d72-40e9-baab-27a88240204e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ResponseOutputMessage(id='msg_6876b3330e088198912d396a469f42770cf925ef713baf44', content=[ResponseOutputText(annotations=[], text='The current temperature in New Jersey is 11.7°C. If you need more specific weather details like conditions or a forecast, please let me know!', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6faa16-84f2-40fa-89f8-12642ac17daf",
   "metadata": {},
   "source": [
    "## Q2. Adding another tool\n",
    "___\n",
    "Let's add another tool - a function that can add weather data to our database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "87b08486-9ec7-4698-a83c-91f609986e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_weather(city: str, temp: float) -> None:\n",
    "    city = city.strip().lower()\n",
    "    known_weather_data[city] = temp\n",
    "    return 'OK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "58db93d8-8813-4ee5-b723-22602ec93333",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_weather_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"set_weather\",\n",
    "    \"description\": \"Set or update the current temperature for a given city.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"city\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The name of the city to update the weather for.\"\n",
    "            },\n",
    "            \"temp\": {\n",
    "                \"type\": \"number\",\n",
    "                \"description\": \"The temperature to set for the city (in degrees Celsius).\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"city\", \"temp\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6bd45d-5090-4534-8c81-6a8d87776ece",
   "metadata": {},
   "source": [
    "## Update Your System Prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "afe3b0e3-b106-4096-8520-6bd3f6532fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are an intelligent assistant capable of reasoning and calling tools as needed.\n",
    "\n",
    "You have access to two tools:\n",
    "\n",
    "1. `get_weather`: Use this tool to get current weather information for a city. It accepts:\n",
    "   - `query` (string): The city name.\n",
    "\n",
    "2. `set_weather`: Use this tool to set or update the temperature for a city. It accepts:\n",
    "   - `city` (string): The name of the city.\n",
    "   - `temp` (number): The temperature to set, in degrees Celsius.\n",
    "\n",
    "Use `weather` when the user asks for weather information.\n",
    "Use `set_weather` when the user wants to define or update the weather for a city.\n",
    "Respond clearly and helpfully after calling a function.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dc5945-91ae-45dd-b235-3e1c09c2668d",
   "metadata": {},
   "source": [
    "## Update the tools list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f4d1549a-096f-4cae-a40a-b5eb39d48909",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.append(set_weather_tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51b5a06-6010-43f9-8e1a-5b30cf09d53e",
   "metadata": {},
   "source": [
    "## Test calling of the new tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "94264e5b-0e0d-4b41-b762-be439245966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"The current temperature in New Jersey is 31 degree celcious\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d1fc45-629c-48be-8759-c95dda34d585",
   "metadata": {},
   "source": [
    "## Build new chat message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d4e4260f-c03d-49e9-8abc-21bd7fe504e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agent chat message\n",
    "chat_messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_input}\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ea412721-f82b-439a-bee0-a93b5fc08a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ResponseFunctionToolCall(arguments='{\"city\":\"New Jersey\",\"temp\":31}', call_id='call_SguAy4dtmSJF3mhglWi1comF', name='set_weather', type='function_call', id='fc_6876b59f03f4819bab3646faa42111b00e7ff663d063834b', status='completed')]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the LLM API\n",
    "response = client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=chat_messages,\n",
    "    tools=tools\n",
    ")\n",
    "response.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "86d3ae05-5de8-4a05-9af0-61784d4aa752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ResponseOutputMessage(id='msg_6876b6532f54819bb4499b25d6ee38b10e7ff663d063834b', content=[ResponseOutputText(annotations=[], text=\"I've updated the temperature in New Jersey to 31 degrees Celsius. If you need any more assistance, feel free to ask!\", type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_responses =  response.output\n",
    "function_names = []\n",
    "arguments = []\n",
    "functions_result = []\n",
    "\n",
    "for response_ in llm_responses:\n",
    "    response_type = response_.type\n",
    "    response_id = response_.call_id\n",
    "\n",
    "    if response_type == \"function_call\":\n",
    "        function_names.append(response_.name)\n",
    "        arguments.append(json.loads(response_.arguments))\n",
    "\n",
    "\n",
    "for function_name, argument in zip( function_names, arguments):\n",
    "    function = globals()[function_name]\n",
    "    result = function(**argument)\n",
    "    functions_result.append(result)\n",
    "    print(f\"Function call: {function_name}, argument: {argument}\\nresult: {result}\")\n",
    "\n",
    "for response_, result in zip(llm_responses, functions_result):\n",
    "    chat_messages.append(response_)\n",
    "    \n",
    "    chat_messages.append({\n",
    "        \"type\": \"function_call_output\",\n",
    "        \"call_id\": response_.call_id,\n",
    "        \"output\": json.dumps(result),\n",
    "    })\n",
    "\n",
    "response = client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=chat_messages,\n",
    "    tools=tools\n",
    ")\n",
    "\n",
    "response.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1dcafe-4753-4155-8258-fcd48c412431",
   "metadata": {},
   "source": [
    "## MCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e1f0e97c-b7cb-4dfc-a636-6fc4d0fa7b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[39;49m2.10.5\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!fastmcp --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52cd780-bb3f-4dce-8fc2-0ca7f16b59ee",
   "metadata": {},
   "source": [
    "## Q4. Simple MCP Server\n",
    "A simple MCP server from the documentation looks like that:\n",
    "```python\n",
    "# weather_server.py\n",
    "# server.py\n",
    "from fastmcp import FastMCP\n",
    "\n",
    "mcp = FastMCP(\"Demo 🚀\")\n",
    "\n",
    "@mcp.tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers\"\"\"\n",
    "    return a + b\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0aeb091f-e8fd-46e4-8566-10fb617557e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">╭─ FastMCP 2.0 ──────────────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>                                                                            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>    <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">    _ __ ___ ______           __  __  _____________    ____    ____ </span>    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>    <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">   _ __ ___ / ____/___ ______/ /_/  |/  / ____/ __ \\  |___ \\  / __ \\</span>    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>    <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">  _ __ ___ / /_  / __ `/ ___/ __/ /|_/ / /   / /_/ /  ___/ / / / / /</span>    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>    <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> _ __ ___ / __/ / /_/ (__  ) /_/ /  / / /___/ ____/  /  __/_/ /_/ / </span>    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>    <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">_ __ ___ /_/    \\__,_/____/\\__/_/  /_/\\____/_/      /_____(_)____/  </span>    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>                                                                            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>                                                                            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>                                                                            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>    <span style=\"font-weight: bold\">🖥️  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Server name:     </span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Demo 🚀              </span>                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>    <span style=\"font-weight: bold\">📦 </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Transport:       </span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">STDIO                </span>                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>    <span style=\"font-weight: bold\">   </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">                 </span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                     </span>                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>    <span style=\"font-weight: bold\">📚 </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Docs:            </span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">https://gofastmcp.com</span>                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>    <span style=\"font-weight: bold\">🚀 </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Deploy:          </span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">https://fastmcp.cloud</span>                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>    <span style=\"font-weight: bold\">   </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">                 </span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                     </span>                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>    <span style=\"font-weight: bold\">🏎️  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">FastMCP version: </span><span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">2.10.5               </span>                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>    <span style=\"font-weight: bold\">🤝 </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">MCP version:     </span><span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">1.11.0               </span>                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>                                                                            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">╰────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\n",
       "\u001b[2m╭─\u001b[0m\u001b[2m FastMCP 2.0 \u001b[0m\u001b[2m─────────────────────────────────────────────────────────────\u001b[0m\u001b[2m─╮\u001b[0m\n",
       "\u001b[2m│\u001b[0m                                                                            \u001b[2m│\u001b[0m\n",
       "\u001b[2m│\u001b[0m    \u001b[1;32m    _ __ ___ ______           __  __  _____________    ____    ____ \u001b[0m    \u001b[2m│\u001b[0m\n",
       "\u001b[2m│\u001b[0m    \u001b[1;32m   _ __ ___ / ____/___ ______/ /_/  |/  / ____/ __ \\  |___ \\  / __ \\\u001b[0m    \u001b[2m│\u001b[0m\n",
       "\u001b[2m│\u001b[0m    \u001b[1;32m  _ __ ___ / /_  / __ `/ ___/ __/ /|_/ / /   / /_/ /  ___/ / / / / /\u001b[0m    \u001b[2m│\u001b[0m\n",
       "\u001b[2m│\u001b[0m    \u001b[1;32m _ __ ___ / __/ / /_/ (__  ) /_/ /  / / /___/ ____/  /  __/_/ /_/ / \u001b[0m    \u001b[2m│\u001b[0m\n",
       "\u001b[2m│\u001b[0m    \u001b[1;32m_ __ ___ /_/    \\__,_/____/\\__/_/  /_/\\____/_/      /_____(_)____/  \u001b[0m    \u001b[2m│\u001b[0m\n",
       "\u001b[2m│\u001b[0m                                                                            \u001b[2m│\u001b[0m\n",
       "\u001b[2m│\u001b[0m                                                                            \u001b[2m│\u001b[0m\n",
       "\u001b[2m│\u001b[0m                                                                            \u001b[2m│\u001b[0m\n",
       "\u001b[2m│\u001b[0m    \u001b[1m🖥️ \u001b[0m\u001b[1m \u001b[0m\u001b[1;36mServer name:    \u001b[0m\u001b[1;36m \u001b[0m\u001b[37mDemo 🚀              \u001b[0m                               \u001b[2m│\u001b[0m\n",
       "\u001b[2m│\u001b[0m    \u001b[1m📦\u001b[0m\u001b[1m \u001b[0m\u001b[1;36mTransport:      \u001b[0m\u001b[1;36m \u001b[0m\u001b[37mSTDIO                \u001b[0m                               \u001b[2m│\u001b[0m\n",
       "\u001b[2m│\u001b[0m    \u001b[1m  \u001b[0m\u001b[1m \u001b[0m\u001b[1;36m                \u001b[0m\u001b[1;36m \u001b[0m\u001b[37m                     \u001b[0m                               \u001b[2m│\u001b[0m\n",
       "\u001b[2m│\u001b[0m    \u001b[1m📚\u001b[0m\u001b[1m \u001b[0m\u001b[1;36mDocs:           \u001b[0m\u001b[1;36m \u001b[0m\u001b[37mhttps://gofastmcp.com\u001b[0m                               \u001b[2m│\u001b[0m\n",
       "\u001b[2m│\u001b[0m    \u001b[1m🚀\u001b[0m\u001b[1m \u001b[0m\u001b[1;36mDeploy:         \u001b[0m\u001b[1;36m \u001b[0m\u001b[37mhttps://fastmcp.cloud\u001b[0m                               \u001b[2m│\u001b[0m\n",
       "\u001b[2m│\u001b[0m    \u001b[1m  \u001b[0m\u001b[1m \u001b[0m\u001b[1;36m                \u001b[0m\u001b[1;36m \u001b[0m\u001b[37m                     \u001b[0m                               \u001b[2m│\u001b[0m\n",
       "\u001b[2m│\u001b[0m    \u001b[1m🏎️ \u001b[0m\u001b[1m \u001b[0m\u001b[1;36mFastMCP version:\u001b[0m\u001b[1;36m \u001b[0m\u001b[2;37m2.10.5               \u001b[0m                               \u001b[2m│\u001b[0m\n",
       "\u001b[2m│\u001b[0m    \u001b[1m🤝\u001b[0m\u001b[1m \u001b[0m\u001b[1;36mMCP version:    \u001b[0m\u001b[1;36m \u001b[0m\u001b[2;37m1.11.0               \u001b[0m                               \u001b[2m│\u001b[0m\n",
       "\u001b[2m│\u001b[0m                                                                            \u001b[2m│\u001b[0m\n",
       "\u001b[2m╰────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'OutStream' object has no attribute 'buffer'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[94]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mOK\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# 🚀 Run the MCP server in Jupyter\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m mcp.run_async()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/LLM-Projects/LLM-Zoomcamp/LLM-workSpace/venv/lib/python3.11/site-packages/fastmcp/server/server.py:303\u001b[39m, in \u001b[36mFastMCP.run_async\u001b[39m\u001b[34m(self, transport, show_banner, **transport_kwargs)\u001b[39m\n\u001b[32m    300\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnknown transport: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransport\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m transport == \u001b[33m\"\u001b[39m\u001b[33mstdio\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m303\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.run_stdio_async(\n\u001b[32m    304\u001b[39m         show_banner=show_banner,\n\u001b[32m    305\u001b[39m         **transport_kwargs,\n\u001b[32m    306\u001b[39m     )\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m transport \u001b[38;5;129;01min\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mhttp\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msse\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstreamable-http\u001b[39m\u001b[33m\"\u001b[39m}:\n\u001b[32m    308\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.run_http_async(\n\u001b[32m    309\u001b[39m         transport=transport,\n\u001b[32m    310\u001b[39m         show_banner=show_banner,\n\u001b[32m    311\u001b[39m         **transport_kwargs,\n\u001b[32m    312\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/LLM-Projects/LLM-Zoomcamp/LLM-workSpace/venv/lib/python3.11/site-packages/fastmcp/server/server.py:1370\u001b[39m, in \u001b[36mFastMCP.run_stdio_async\u001b[39m\u001b[34m(self, show_banner)\u001b[39m\n\u001b[32m   1364\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m show_banner:\n\u001b[32m   1365\u001b[39m     log_server_banner(\n\u001b[32m   1366\u001b[39m         server=\u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1367\u001b[39m         transport=\u001b[33m\"\u001b[39m\u001b[33mstdio\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1368\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1370\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m stdio_server() \u001b[38;5;28;01mas\u001b[39;00m (read_stream, write_stream):\n\u001b[32m   1371\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStarting MCP server \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m with transport \u001b[39m\u001b[33m'\u001b[39m\u001b[33mstdio\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1372\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._mcp_server.run(\n\u001b[32m   1373\u001b[39m         read_stream,\n\u001b[32m   1374\u001b[39m         write_stream,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1377\u001b[39m         ),\n\u001b[32m   1378\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.9/lib/python3.11/contextlib.py:210\u001b[39m, in \u001b[36m_AsyncGeneratorContextManager.__aenter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.kwds, \u001b[38;5;28mself\u001b[39m.func\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m anext(\u001b[38;5;28mself\u001b[39m.gen)\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m:\n\u001b[32m    212\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mgenerator didn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt yield\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/LLM-Projects/LLM-Zoomcamp/LLM-workSpace/venv/lib/python3.11/site-packages/mcp/server/stdio.py:49\u001b[39m, in \u001b[36mstdio_server\u001b[39m\u001b[34m(stdin, stdout)\u001b[39m\n\u001b[32m     47\u001b[39m     stdin = anyio.wrap_file(TextIOWrapper(sys.stdin.buffer, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stdout:\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     stdout = anyio.wrap_file(TextIOWrapper(\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuffer\u001b[49m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     51\u001b[39m read_stream: MemoryObjectReceiveStream[SessionMessage | \u001b[38;5;167;01mException\u001b[39;00m]\n\u001b[32m     52\u001b[39m read_stream_writer: MemoryObjectSendStream[SessionMessage | \u001b[38;5;167;01mException\u001b[39;00m]\n",
      "\u001b[31mAttributeError\u001b[39m: 'OutStream' object has no attribute 'buffer'"
     ]
    }
   ],
   "source": [
    "from fastmcp import FastMCP\n",
    "import random\n",
    "\n",
    "# Local state\n",
    "known_weather_data = {}\n",
    "\n",
    "# Create FastMCP instance\n",
    "mcp = FastMCP(\"Demo 🚀\")\n",
    "\n",
    "# Define tools\n",
    "@mcp.tool\n",
    "def get_weather(city: str) -> float:\n",
    "    \"\"\"\n",
    "    Retrieves the temperature for a specified city.\n",
    "\n",
    "    Parameters:\n",
    "        city (str): The name of the city for which to retrieve weather data.\n",
    "\n",
    "    Returns:\n",
    "        float: The temperature associated with the city.\n",
    "    \"\"\"\n",
    "    city = city.strip().lower()\n",
    "\n",
    "    if city in known_weather_data:\n",
    "        return known_weather_data[city]\n",
    "\n",
    "    return round(random.uniform(-5, 35), 1)\n",
    "\n",
    "@mcp.tool\n",
    "def set_weather(city: str, temp: float) -> str:\n",
    "    \"\"\"\n",
    "    Sets the temperature for a specified city.\n",
    "\n",
    "    Parameters:\n",
    "        city (str): The name of the city for which to set the weather data.\n",
    "        temp (float): The temperature to associate with the city.\n",
    "\n",
    "    Returns:\n",
    "        str: A confirmation string 'OK' indicating successful update.\n",
    "    \"\"\"\n",
    "    city = city.strip().lower()\n",
    "    known_weather_data[city] = temp\n",
    "    return 'OK'\n",
    "\n",
    "# 🚀 Run the MCP server in Jupyter\n",
    "await mcp.run_async()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d2cc8509-9bc5-46b5-ac63-9618a4e8f4cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Already running asyncio in this thread",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[93]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmcp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/LLM-Projects/LLM-Zoomcamp/LLM-workSpace/venv/lib/python3.11/site-packages/fastmcp/server/server.py:328\u001b[39m, in \u001b[36mFastMCP.run\u001b[39m\u001b[34m(self, transport, show_banner, **transport_kwargs)\u001b[39m\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\n\u001b[32m    317\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    318\u001b[39m     transport: Transport | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    319\u001b[39m     show_banner: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    320\u001b[39m     **transport_kwargs: Any,\n\u001b[32m    321\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    322\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Run the FastMCP server. Note this is a synchronous function.\u001b[39;00m\n\u001b[32m    323\u001b[39m \n\u001b[32m    324\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    325\u001b[39m \u001b[33;03m        transport: Transport protocol to use (\"stdio\", \"sse\", or \"streamable-http\")\u001b[39;00m\n\u001b[32m    326\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m     \u001b[43manyio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_async\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtransport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m            \u001b[49m\u001b[43mshow_banner\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_banner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtransport_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/LLM-Projects/LLM-Zoomcamp/LLM-workSpace/venv/lib/python3.11/site-packages/anyio/_core/_eventloop.py:59\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(func, backend, backend_options, *args)\u001b[39m\n\u001b[32m     57\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAlready running \u001b[39m\u001b[38;5;132;01m{\u001b[39;00masynclib_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in this thread\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     async_backend = get_async_backend(backend)\n",
      "\u001b[31mRuntimeError\u001b[39m: Already running asyncio in this thread"
     ]
    }
   ],
   "source": [
    "mcp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14d0f88-e87e-482e-8817-85b2f952c45a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
